{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a015d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1b03d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import operator\n",
    "from typing import TypedDict, List, Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb304740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "    brief: str = Field(..., description=\"What to cover\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04e5f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    tasks: List[Task]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa228a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    plan: Plan\n",
    "    # reducer: results from workers get concatenated automatically\n",
    "    sections: Annotated[List[str], operator.add]\n",
    "    final: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e1111f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State) -> dict:\n",
    "\n",
    "    plan = llm.with_structured_output(Plan).invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=(\n",
    "                    \"Create a blog plan with 3-5 sections on the following topic.\"\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(content=f\"Topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "    return {\"plan\": plan}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a2fc3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fanout(state: State):\n",
    "    return [Send(\"worker\", {\"task\": task, \"topic\": state[\"topic\"], \"plan\": state[\"plan\"]})\n",
    "            for task in state[\"plan\"].tasks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5c35814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(payload: dict) -> dict:\n",
    "\n",
    "    # payload contains what we sent\n",
    "    task = payload[\"task\"]\n",
    "    topic = payload[\"topic\"]\n",
    "    plan = payload[\"plan\"]\n",
    "\n",
    "    blog_title = plan.blog_title\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Write one clean Markdown section.\"),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog: {blog_title}\\n\"\n",
    "                    f\"Topic: {topic}\\n\\n\"\n",
    "                    f\"Section: {task.title}\\n\"\n",
    "                    f\"Brief: {task.brief}\\n\\n\"\n",
    "                    \"Return only the section content in Markdown.\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    return {\"sections\": [section_md]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f04cf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000024A6DDB3110>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def reducer(state: State) -> dict:\n",
    "    \n",
    "    title = state[\"plan\"].blog_title\n",
    "    body = \"\\n\\n\".join(state[\"sections\"]).strip()\n",
    "\n",
    "    final_md = f\"# {title}\\n\\n{body}\\n\"\n",
    "\n",
    "    # ---- save to file ----\n",
    "    filename = title.lower().replace(\" \", \"_\") + \".md\"\n",
    "    output_path = Path(filename)\n",
    "    output_path.write_text(final_md, encoding=\"utf-8\")\n",
    "\n",
    "    return {\"final\": final_md}\n",
    "g = StateGraph(State)\n",
    "g.add_node(\"orchestrator\", orchestrator)\n",
    "g.add_node(\"worker\", worker)\n",
    "g.add_node(\"reducer\", reducer)\n",
    "g.add_edge(START, \"orchestrator\")\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "g.add_edge(\"worker\", \"reducer\")\n",
    "g.add_edge(\"reducer\", END)\n",
    "\n",
    "app = g.compile()\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "729723e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = app.invoke({\"topic\": \"Write a blog on Self Attention\", \"sections\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acf48cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Write a blog on Self Attention',\n",
       " 'plan': Plan(blog_title='Self-Attention Explained: How AI Understands Context', tasks=[Task(id=1, title='Introduction to Attention Mechanisms', brief='Introduce the general concept of attention in deep learning, its purpose in helping models focus on relevant parts of input, and set the stage for self-attention as a powerful evolution.'), Task(id=2, title=\"The 'Self' in Self-Attention: Why It Matters\", brief='Explain what distinguishes self-attention from traditional attention mechanisms. Emphasize its ability to relate different positions of a single sequence to each other to compute a representation of the same sequence, highlighting its importance for understanding context and long-range dependencies.'), Task(id=3, title='Deconstructing Self-Attention: Q, K, V in Action', brief='Detail the core mechanics of self-attention. Explain the roles of Query (Q), Key (K), and Value (V) vectors, how they are derived from input embeddings, and the mathematical steps involved (dot product, scaling, softmax, weighted sum) to compute the attention output.'), Task(id=4, title='Multi-Head Attention: Enhancing Representation', brief=\"Introduce Multi-Head Attention as an extension of single-head self-attention. Explain its purpose: allowing the model to jointly attend to information from different representation subspaces at different positions, thereby enriching the model's understanding and capturing diverse relationships.\"), Task(id=5, title=\"Self-Attention's Triumph: The Transformer Architecture\", brief='Discuss the pivotal role of self-attention (and Multi-Head Attention) in the Transformer architecture. Highlight how Transformers revolutionized NLP (e.g., BERT, GPT) and their growing influence in other domains like computer vision, all thanks to self-attention.'), Task(id=6, title='Advantages and Considerations of Self-Attention', brief='Summarize the key benefits of self-attention, such as its ability to capture long-range dependencies, parallelizability, and potential for interpretability. Also, address potential challenges like computational cost for very long sequences and memory requirements.'), Task(id=7, title='Conclusion: The Future of Context-Aware AI', brief='Recap the transformative impact of self-attention on modern AI. Briefly touch upon ongoing research, variations, and its continued importance as a foundational building block for more intelligent and context-aware models.')]),\n",
       " 'sections': ['## Introduction to Attention Mechanisms\\n\\nIn the vast landscape of deep learning, models often grapple with the challenge of processing lengthy sequences of information, be it sentences, time-series data, or images. Traditional architectures, like recurrent neural networks (RNNs), often struggle to retain information from the beginning of a long sequence by the time they reach the end, leading to a \"bottleneck\" where crucial context can be lost. This is where the concept of \"attention\" revolutionized the field.\\n\\nInspired by how humans naturally focus on specific parts of a scene or sentence to understand its meaning, attention mechanisms allow a neural network to dynamically weigh the importance of different parts of its input when making a prediction or generating an output. Instead of trying to compress all information into a single, fixed-size vector, attention enables the model to look back at the original input and selectively pick out the most relevant pieces of information at each step. This dramatically improves the model\\'s ability to handle long-range dependencies and maintain context. It\\'s akin to highlighting the most important phrases in a document while reading – you\\'re not just scanning, you\\'re actively focusing your cognitive resources.\\n\\nWhile the initial forms of attention significantly boosted the performance of sequence-to-sequence models (like those used in machine translation), they often still relied on external context from a decoder. The true power unleashed came with the advent of **Self-Attention**, a groundbreaking evolution that allows the model to weigh the importance of different parts of the *same* input sequence against each other, internally deriving relationships and context without external guidance. This elegant mechanism is a cornerstone of modern AI, particularly in the transformer architecture, and is what truly enables models to \"understand\" context in a profound new way.',\n",
       "  '## The \\'Self\\' in Self-Attention: Why It Matters\\n\\nWhat makes \"self-attention\" so revolutionary, and where does that \"self\" come from? Traditional attention mechanisms often connect two *different* sequences – for example, a query from a target sentence looking back at a source sentence in machine translation. Self-attention, however, turns this inward. It\\'s about relating different positions of a *single sequence* to each other to compute a representation of that *same sequence*.\\n\\nImagine the AI is reading a sentence like \"The bank was so steep, I couldn\\'t climb it.\" For the word \"bank,\" self-attention allows the model to simultaneously look at and weigh the importance of \"steep\" and \"climb\" within the *very same sentence*. This internal introspection is crucial. It enables the model to explicitly understand how each word, or token, in an input sequence relates to every other word in that identical sequence. This capacity to build rich, context-aware representations directly addresses the challenge of understanding context and capturing long-range dependencies that traditional recurrent neural networks often struggled with. By creating direct connections between any two positions, no matter how far apart, self-attention empowers models to grasp the nuances and relationships across an entire input with unprecedented clarity.',\n",
       "  '## Deconstructing Self-Attention: Q, K, V in Action\\n\\nAt the heart of self-attention lies a deceptively simple yet powerful mechanism built around three core vectors: Query (Q), Key (K), and Value (V). Think of them as fundamental components in a sophisticated information retrieval system, where each word in a sentence is trying to find relevant information from all other words (including itself) to better understand its own context.\\n\\n**1. The Genesis: From Embeddings to Q, K, V**\\n\\nEvery word in our input sentence first gets transformed into a numerical representation called an **embedding**. This embedding is a dense vector capturing semantic meaning. For each individual word\\'s embedding, we then generate its corresponding Q, K, and V vectors. This is achieved by multiplying the input embedding by three distinct, trainable weight matrices: $W_Q$, $W_K$, and $W_V$.\\n\\n*   **Query (Q) Vector**: Represents \"what I\\'m looking for.\" For the current word, its Q vector is used to query all other words in the sequence.\\n*   **Key (K) Vector**: Represents \"what I have to offer.\" For every word in the sequence, its K vector acts like a label or identifier that can be queried.\\n*   **Value (V) Vector**: Represents \"the information I carry.\" For every word, its V vector contains the actual content or information that will be passed on if its Key is deemed relevant.\\n\\nCrucially, these $W_Q$, $W_K$, and $W_V$ matrices are learned during the training process. They allow the model to project the original word embeddings into different representational spaces specifically optimized for querying, matching, and extracting information.\\n\\n**2. The Attention Score: Querying and Matching (Dot Product & Scaling)**\\n\\nOnce we have our Q, K, and V vectors for all words, the self-attention mechanism begins to compute relevance. For a specific word (let\\'s call it the \"query word\"), its Q vector is compared against the K vector of *every* word in the input sequence (including itself).\\n\\nThe comparison is typically done using a **dot product**:\\n\\n$Score(QueryWord, KeyWord) = Q_{QueryWord} \\\\cdot K_{KeyWord}$\\n\\nA higher dot product score indicates greater similarity or relevance between the query word and the key word. This score tells us how much attention the query word should pay to the key word.\\n\\nTo prevent the dot product values from becoming too large and pushing the softmax function into regions with extremely small gradients, these scores are then **scaled** by dividing them by the square root of the dimension of the key vectors ($\\\\sqrt{d_k}$):\\n\\n$ScaledScore = Score(QueryWord, KeyWord) / \\\\sqrt{d_k}$\\n\\n**3. Normalization: Softmax for Probability Distribution**\\n\\nThe scaled scores for a single query word (comparing it against all keys) are then passed through a **softmax function**. Softmax converts these raw scores into a probability distribution, ensuring that all attention weights for a given query word sum up to 1.\\n\\n$AttentionWeights = Softmax(ScaledScores)$\\n\\nThese attention weights now explicitly tell us, for our query word, what percentage of attention it should allocate to each word in the sequence. A word with a high attention weight is considered highly relevant to the query word\\'s context.\\n\\n**4. The Output: Weighted Sum of Values**\\n\\nFinally, armed with these attention weights, we compute the output for our query word. This is done by taking a **weighted sum of all the Value (V) vectors** in the sequence, where the weights are the attention weights calculated in the previous step:\\n\\n$Output_{QueryWord} = \\\\sum_{i=1}^{SequenceLength} AttentionWeights_i \\\\cdot V_i$\\n\\nThis weighted sum effectively aggregates information from all relevant words based on their importance. If a word has a high attention weight, its Value vector contributes more significantly to the final output vector for the query word. Conversely, words with low attention weights contribute very little.\\n\\nThis entire process is performed in parallel for every word in the input sequence, allowing the model to generate a new contextualized representation for each word, incorporating information from the entire sequence based on learned relevance. This is how \"understanding context\" is mathematically achieved within the self-attention mechanism.',\n",
       "  '## Multi-Head Attention: Enhancing Representation\\n\\nWhile a single self-attention \"head\" is powerful, it has limitations. It focuses on one way of relating words to each other. To enrich the model\\'s understanding and capture a wider spectrum of relationships, we introduce **Multi-Head Attention**. This mechanism is essentially an extension where, instead of performing self-attention once, the input is fed into *multiple* self-attention heads in parallel.\\n\\nEach of these \"heads\" learns to project the input into different, independent representation subspaces. This means that each head can learn to focus on different aspects of the input or different types of relationships between words. For instance, one head might learn to identify subject-verb relationships, while another might focus on adjective-noun pairings, and yet another on temporal dependencies. The outputs from these independent attention heads are then concatenated and linearly transformed back into the desired dimension.\\n\\nBy allowing the model to jointly attend to information from these different representation subspaces at different positions, Multi-Head Attention significantly enhances the model\\'s ability to capture diverse relationships and nuances within the data. This parallel processing of distinct \"perspectives\" dramatically enriches the overall contextual understanding, making the model far more robust and insightful.',\n",
       "  '## Self-Attention\\'s Triumph: The Transformer Architecture\\n\\nWhile the concept of attention had existed, its true power was unleashed with the advent of the Transformer architecture in 2017. The Transformer, a groundbreaking neural network design, completely sidestepped traditional recurrent (RNNs) and convolutional (CNNs) layers, relying solely on self-attention mechanisms to process sequences. This bold architectural choice proved to be a stroke of genius, fundamentally reshaping the landscape of artificial intelligence.\\n\\nAt the heart of the Transformer\\'s success is self-attention, particularly its more sophisticated cousin, Multi-Head Attention. Multi-Head Attention allows the model to simultaneously attend to different parts of the input sequence from multiple \"perspectives\" or \"representation subspaces.\" Each \"head\" learns to focus on different types of relationships or dependencies within the data, providing a richer, more nuanced understanding of context than a single attention mechanism could achieve. This parallel processing capability also made Transformers significantly more efficient to train on modern hardware, overcoming the sequential bottlenecks of RNNs.\\n\\nThe impact was immediate and profound, especially in Natural Language Processing (NLP). Transformers became the backbone for revolutionary models like Google\\'s BERT (Bidirectional Encoder Representations from Transformers) and OpenAI\\'s GPT (Generative Pre-trained Transformer) series. These models, powered by self-attention, demonstrated unprecedented capabilities in understanding, generating, and translating human language, achieving state-of-the-art results across a vast array of NLP tasks. They paved the way for more human-like conversations with AI, sophisticated content creation, and highly accurate language translation.\\n\\nThe success of self-attention within the Transformer architecture wasn\\'t confined to text. Its ability to model long-range dependencies and capture intricate relationships between elements has led to its growing influence in other domains. Transformers are now making significant inroads into computer vision, powering models for image recognition, object detection, and even video analysis, demonstrating that the triumph of self-attention is a universal one, applicable to diverse forms of sequential and relational data.',\n",
       "  '## Advantages and Considerations of Self-Attention\\n\\nSelf-attention brings a paradigm shift in how AI processes sequential data, offering significant advantages over traditional recurrent and convolutional architectures. Firstly, its unparalleled ability to **capture long-range dependencies** is a game-changer. Unlike LSTMs or GRUs that struggle to remember information from very distant parts of a sequence, self-attention directly computes relationships between any two tokens, regardless of their position, allowing for a much richer contextual understanding.\\n\\nSecondly, self-attention is inherently **parallelizable**. Since the attention mechanism calculates relationships for all tokens simultaneously, it can be processed efficiently on modern hardware like GPUs, leading to faster training times compared to sequential processing in RNNs. This parallelization is a key factor in the scalability of large transformer models. Furthermore, the attention weights themselves offer a degree of **interpretability**. By visualizing which parts of the input a token \"attends\" to, researchers and developers can gain insights into how the model makes its decisions, providing a valuable tool for debugging and understanding complex AI systems.\\n\\nHowever, self-attention is not without its considerations. The primary challenge lies in its **computational cost and memory requirements**, particularly for very long sequences. The attention mechanism calculates an interaction matrix between every token and every other token, leading to a quadratic complexity ($O(N^2)$) with respect to the sequence length ($N$). This can become prohibitively expensive for extremely long texts, high-resolution images, or extensive audio sequences, requiring substantial computational power and memory. Researchers are actively exploring more efficient variants, such as sparse attention or linear attention, to mitigate these limitations and extend the applicability of self-attention to even larger scales.',\n",
       "  \"## Conclusion: The Future of Context-Aware AI\\n\\nSelf-attention has undeniably revolutionized modern AI, particularly in the realm of natural language processing and beyond. By empowering models to weigh the importance of different parts of an input sequence relative to each other, it shattered previous limitations, enabling sophisticated understanding of context, nuance, and long-range dependencies. This breakthrough fueled the extraordinary success of Transformers, leading to remarkable advancements in machine translation, text generation, image recognition, and countless other applications that once seemed distant.\\n\\nThe journey doesn't end here. Research continues at a rapid pace, exploring variations like sparse attention, linear attention, and various efficiency improvements to handle ever-larger models and longer sequences. Furthermore, self-attention remains a foundational building block, constantly being integrated into novel architectures and domains, pushing the boundaries of what AI can achieve. As we strive for more intelligent, robust, and truly context-aware AI systems, self-attention's core mechanism will undoubtedly remain a cornerstone, enabling machines to perceive and interpret the world with an ever-deepening understanding.\"],\n",
       " 'final': '# Self-Attention Explained: How AI Understands Context\\n\\n## Introduction to Attention Mechanisms\\n\\nIn the vast landscape of deep learning, models often grapple with the challenge of processing lengthy sequences of information, be it sentences, time-series data, or images. Traditional architectures, like recurrent neural networks (RNNs), often struggle to retain information from the beginning of a long sequence by the time they reach the end, leading to a \"bottleneck\" where crucial context can be lost. This is where the concept of \"attention\" revolutionized the field.\\n\\nInspired by how humans naturally focus on specific parts of a scene or sentence to understand its meaning, attention mechanisms allow a neural network to dynamically weigh the importance of different parts of its input when making a prediction or generating an output. Instead of trying to compress all information into a single, fixed-size vector, attention enables the model to look back at the original input and selectively pick out the most relevant pieces of information at each step. This dramatically improves the model\\'s ability to handle long-range dependencies and maintain context. It\\'s akin to highlighting the most important phrases in a document while reading – you\\'re not just scanning, you\\'re actively focusing your cognitive resources.\\n\\nWhile the initial forms of attention significantly boosted the performance of sequence-to-sequence models (like those used in machine translation), they often still relied on external context from a decoder. The true power unleashed came with the advent of **Self-Attention**, a groundbreaking evolution that allows the model to weigh the importance of different parts of the *same* input sequence against each other, internally deriving relationships and context without external guidance. This elegant mechanism is a cornerstone of modern AI, particularly in the transformer architecture, and is what truly enables models to \"understand\" context in a profound new way.\\n\\n## The \\'Self\\' in Self-Attention: Why It Matters\\n\\nWhat makes \"self-attention\" so revolutionary, and where does that \"self\" come from? Traditional attention mechanisms often connect two *different* sequences – for example, a query from a target sentence looking back at a source sentence in machine translation. Self-attention, however, turns this inward. It\\'s about relating different positions of a *single sequence* to each other to compute a representation of that *same sequence*.\\n\\nImagine the AI is reading a sentence like \"The bank was so steep, I couldn\\'t climb it.\" For the word \"bank,\" self-attention allows the model to simultaneously look at and weigh the importance of \"steep\" and \"climb\" within the *very same sentence*. This internal introspection is crucial. It enables the model to explicitly understand how each word, or token, in an input sequence relates to every other word in that identical sequence. This capacity to build rich, context-aware representations directly addresses the challenge of understanding context and capturing long-range dependencies that traditional recurrent neural networks often struggled with. By creating direct connections between any two positions, no matter how far apart, self-attention empowers models to grasp the nuances and relationships across an entire input with unprecedented clarity.\\n\\n## Deconstructing Self-Attention: Q, K, V in Action\\n\\nAt the heart of self-attention lies a deceptively simple yet powerful mechanism built around three core vectors: Query (Q), Key (K), and Value (V). Think of them as fundamental components in a sophisticated information retrieval system, where each word in a sentence is trying to find relevant information from all other words (including itself) to better understand its own context.\\n\\n**1. The Genesis: From Embeddings to Q, K, V**\\n\\nEvery word in our input sentence first gets transformed into a numerical representation called an **embedding**. This embedding is a dense vector capturing semantic meaning. For each individual word\\'s embedding, we then generate its corresponding Q, K, and V vectors. This is achieved by multiplying the input embedding by three distinct, trainable weight matrices: $W_Q$, $W_K$, and $W_V$.\\n\\n*   **Query (Q) Vector**: Represents \"what I\\'m looking for.\" For the current word, its Q vector is used to query all other words in the sequence.\\n*   **Key (K) Vector**: Represents \"what I have to offer.\" For every word in the sequence, its K vector acts like a label or identifier that can be queried.\\n*   **Value (V) Vector**: Represents \"the information I carry.\" For every word, its V vector contains the actual content or information that will be passed on if its Key is deemed relevant.\\n\\nCrucially, these $W_Q$, $W_K$, and $W_V$ matrices are learned during the training process. They allow the model to project the original word embeddings into different representational spaces specifically optimized for querying, matching, and extracting information.\\n\\n**2. The Attention Score: Querying and Matching (Dot Product & Scaling)**\\n\\nOnce we have our Q, K, and V vectors for all words, the self-attention mechanism begins to compute relevance. For a specific word (let\\'s call it the \"query word\"), its Q vector is compared against the K vector of *every* word in the input sequence (including itself).\\n\\nThe comparison is typically done using a **dot product**:\\n\\n$Score(QueryWord, KeyWord) = Q_{QueryWord} \\\\cdot K_{KeyWord}$\\n\\nA higher dot product score indicates greater similarity or relevance between the query word and the key word. This score tells us how much attention the query word should pay to the key word.\\n\\nTo prevent the dot product values from becoming too large and pushing the softmax function into regions with extremely small gradients, these scores are then **scaled** by dividing them by the square root of the dimension of the key vectors ($\\\\sqrt{d_k}$):\\n\\n$ScaledScore = Score(QueryWord, KeyWord) / \\\\sqrt{d_k}$\\n\\n**3. Normalization: Softmax for Probability Distribution**\\n\\nThe scaled scores for a single query word (comparing it against all keys) are then passed through a **softmax function**. Softmax converts these raw scores into a probability distribution, ensuring that all attention weights for a given query word sum up to 1.\\n\\n$AttentionWeights = Softmax(ScaledScores)$\\n\\nThese attention weights now explicitly tell us, for our query word, what percentage of attention it should allocate to each word in the sequence. A word with a high attention weight is considered highly relevant to the query word\\'s context.\\n\\n**4. The Output: Weighted Sum of Values**\\n\\nFinally, armed with these attention weights, we compute the output for our query word. This is done by taking a **weighted sum of all the Value (V) vectors** in the sequence, where the weights are the attention weights calculated in the previous step:\\n\\n$Output_{QueryWord} = \\\\sum_{i=1}^{SequenceLength} AttentionWeights_i \\\\cdot V_i$\\n\\nThis weighted sum effectively aggregates information from all relevant words based on their importance. If a word has a high attention weight, its Value vector contributes more significantly to the final output vector for the query word. Conversely, words with low attention weights contribute very little.\\n\\nThis entire process is performed in parallel for every word in the input sequence, allowing the model to generate a new contextualized representation for each word, incorporating information from the entire sequence based on learned relevance. This is how \"understanding context\" is mathematically achieved within the self-attention mechanism.\\n\\n## Multi-Head Attention: Enhancing Representation\\n\\nWhile a single self-attention \"head\" is powerful, it has limitations. It focuses on one way of relating words to each other. To enrich the model\\'s understanding and capture a wider spectrum of relationships, we introduce **Multi-Head Attention**. This mechanism is essentially an extension where, instead of performing self-attention once, the input is fed into *multiple* self-attention heads in parallel.\\n\\nEach of these \"heads\" learns to project the input into different, independent representation subspaces. This means that each head can learn to focus on different aspects of the input or different types of relationships between words. For instance, one head might learn to identify subject-verb relationships, while another might focus on adjective-noun pairings, and yet another on temporal dependencies. The outputs from these independent attention heads are then concatenated and linearly transformed back into the desired dimension.\\n\\nBy allowing the model to jointly attend to information from these different representation subspaces at different positions, Multi-Head Attention significantly enhances the model\\'s ability to capture diverse relationships and nuances within the data. This parallel processing of distinct \"perspectives\" dramatically enriches the overall contextual understanding, making the model far more robust and insightful.\\n\\n## Self-Attention\\'s Triumph: The Transformer Architecture\\n\\nWhile the concept of attention had existed, its true power was unleashed with the advent of the Transformer architecture in 2017. The Transformer, a groundbreaking neural network design, completely sidestepped traditional recurrent (RNNs) and convolutional (CNNs) layers, relying solely on self-attention mechanisms to process sequences. This bold architectural choice proved to be a stroke of genius, fundamentally reshaping the landscape of artificial intelligence.\\n\\nAt the heart of the Transformer\\'s success is self-attention, particularly its more sophisticated cousin, Multi-Head Attention. Multi-Head Attention allows the model to simultaneously attend to different parts of the input sequence from multiple \"perspectives\" or \"representation subspaces.\" Each \"head\" learns to focus on different types of relationships or dependencies within the data, providing a richer, more nuanced understanding of context than a single attention mechanism could achieve. This parallel processing capability also made Transformers significantly more efficient to train on modern hardware, overcoming the sequential bottlenecks of RNNs.\\n\\nThe impact was immediate and profound, especially in Natural Language Processing (NLP). Transformers became the backbone for revolutionary models like Google\\'s BERT (Bidirectional Encoder Representations from Transformers) and OpenAI\\'s GPT (Generative Pre-trained Transformer) series. These models, powered by self-attention, demonstrated unprecedented capabilities in understanding, generating, and translating human language, achieving state-of-the-art results across a vast array of NLP tasks. They paved the way for more human-like conversations with AI, sophisticated content creation, and highly accurate language translation.\\n\\nThe success of self-attention within the Transformer architecture wasn\\'t confined to text. Its ability to model long-range dependencies and capture intricate relationships between elements has led to its growing influence in other domains. Transformers are now making significant inroads into computer vision, powering models for image recognition, object detection, and even video analysis, demonstrating that the triumph of self-attention is a universal one, applicable to diverse forms of sequential and relational data.\\n\\n## Advantages and Considerations of Self-Attention\\n\\nSelf-attention brings a paradigm shift in how AI processes sequential data, offering significant advantages over traditional recurrent and convolutional architectures. Firstly, its unparalleled ability to **capture long-range dependencies** is a game-changer. Unlike LSTMs or GRUs that struggle to remember information from very distant parts of a sequence, self-attention directly computes relationships between any two tokens, regardless of their position, allowing for a much richer contextual understanding.\\n\\nSecondly, self-attention is inherently **parallelizable**. Since the attention mechanism calculates relationships for all tokens simultaneously, it can be processed efficiently on modern hardware like GPUs, leading to faster training times compared to sequential processing in RNNs. This parallelization is a key factor in the scalability of large transformer models. Furthermore, the attention weights themselves offer a degree of **interpretability**. By visualizing which parts of the input a token \"attends\" to, researchers and developers can gain insights into how the model makes its decisions, providing a valuable tool for debugging and understanding complex AI systems.\\n\\nHowever, self-attention is not without its considerations. The primary challenge lies in its **computational cost and memory requirements**, particularly for very long sequences. The attention mechanism calculates an interaction matrix between every token and every other token, leading to a quadratic complexity ($O(N^2)$) with respect to the sequence length ($N$). This can become prohibitively expensive for extremely long texts, high-resolution images, or extensive audio sequences, requiring substantial computational power and memory. Researchers are actively exploring more efficient variants, such as sparse attention or linear attention, to mitigate these limitations and extend the applicability of self-attention to even larger scales.\\n\\n## Conclusion: The Future of Context-Aware AI\\n\\nSelf-attention has undeniably revolutionized modern AI, particularly in the realm of natural language processing and beyond. By empowering models to weigh the importance of different parts of an input sequence relative to each other, it shattered previous limitations, enabling sophisticated understanding of context, nuance, and long-range dependencies. This breakthrough fueled the extraordinary success of Transformers, leading to remarkable advancements in machine translation, text generation, image recognition, and countless other applications that once seemed distant.\\n\\nThe journey doesn\\'t end here. Research continues at a rapid pace, exploring variations like sparse attention, linear attention, and various efficiency improvements to handle ever-larger models and longer sequences. Furthermore, self-attention remains a foundational building block, constantly being integrated into novel architectures and domains, pushing the boundaries of what AI can achieve. As we strive for more intelligent, robust, and truly context-aware AI systems, self-attention\\'s core mechanism will undoubtedly remain a cornerstone, enabling machines to perceive and interpret the world with an ever-deepening understanding.\\n'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa99efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
